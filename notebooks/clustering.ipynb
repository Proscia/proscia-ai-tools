{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Concentriq® Embeddings for unsupervised clustering\n",
    "\n",
    "In this demo/walkthrough we'll utilize the simple Python client developed specifically for Concentriq® Embeddings and secure our access to Concentriq® for Research and Concentriq® Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from utils.client import ClientWrapper as Client\n",
    "from utils import utils\n",
    "\n",
    "email = os.getenv(\"CONCENTRIQ_EMAIL\")\n",
    "pwd = os.getenv(\"CONCENTRIQ_PASSWORD\")\n",
    "endpoint = os.getenv(\"CONCENTRIQ_ENDPOINT_URL\")\n",
    "\n",
    "# To use CPU instead of GPU, set `device` parameter to `\"cpu\"`\n",
    "ce_api_client = Client(url=endpoint, email=email, password=pwd, device=0)\n",
    "ce_api_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding the IMPRESS dataset\n",
    "\n",
    "For this demo we’ve elected to use the [IMPRESS dataset](https://arxiv.org/html/2306.01546v2), a publicly available dataset containing 126 breast H&E and 126 IHC WSIs from 62 female patients with HER2-positive breast cancer and 64 female patients diagnosed with triple-negative breast cancer. All the slides are scanned using a Hamamatsu scanner with 20x magnification. These WSIs are stored in a repository (ID: 1918) in Concentriq® for Research. \n",
    "\n",
    "To get started, we'll submit a job for our entire dataset and request our embeddings at a scale of 1 µm/pixel (mpp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_ids = [1918] # The IMPRESS Dataset\n",
    "ticket_id = ce_api_client.embed_repos(ids=repo_ids, model=\"facebook/dinov2-base\", mpp=1)\n",
    "\n",
    "# Alternatively, load a previously generated embedding\n",
    "# ticket_id = '80c62381-8814-454f-8d9f-1e62518627ba'\n",
    "# print(ticket_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving and Understanding the Embedding Results\n",
    "\n",
    "Now the embedding job is running and Concentriq® Embeddings is running inference with our selected foundation model. With `get_embeddings`, we can check for completed results, and when complete they’ll get pulled down to a local cache. If the cache is already stored locally, `get_embeddings` will load out the results on demand from disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ce_api_client.get_embeddings(ticket_id)\n",
    "print(f\"{ticket_id}: {len(embeddings['images'])} images\")\n",
    "print(f\"{embeddings['images'][0]['model']} - {embeddings['images'][0]['patch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrieved embedding file includes metadata about the embeddings job we submitted and each WSI processed. The embeddings themselves are loaded as a dictionary of tensors with keys in \"Y_X\" format indicating the grid index of the embedded patch.\n",
    "\n",
    "Here we print the information corresponding to each WSI. This includes the parameters supplied along with implicit attributes of the data, like the foundation model’s native patch size, as well as all of the other spatial information associated with each WSI’s tile embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in embeddings['images'][0].items():\n",
    "    if key not in ['embedding']:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " At this point, this dataset has been processed into embeddings: the dataset is lightweight and ready for downstream analysis, be that classification, segmentation, regression, etc. For this example, we will show how to cluster similar embedding vectors and visualize the underlying tiles. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['images'][0]['embedding']['0_0'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch thumbnails for visualization\n",
    "The embeddings api also contains am endpoint for fetching lightweight thumbnail images that allows us to visualize the image tiles associated with each embedding – let’s fetch the thumbnail images and tile those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbnail_ticket_id = ce_api_client.thumbnail_repos(ids=repo_ids)\n",
    "print(thumbnail_ticket_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbnails = ce_api_client.get_thumbnails(thumbnail_ticket_id, load_thumbnails=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link the thumbnails to the embeddings\n",
    "for emb in embeddings['images']:\n",
    "    thumbnail_dict = [thumb for thumb in thumbnails['thumbnails'] if thumb['image_id'] == emb['image_id']][0]\n",
    "    emb.update(thumbnail_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = embeddings['images'][0]\n",
    "tiles = utils.tile_thumbnail(emb)\n",
    "emb['tiles'] = tiles\n",
    "len(tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Clustering of Tile Embeddings\n",
    "Next let’s import some things to allow us to cluster and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let’s do a 2D UMAP projection of the tile embeddings for a single slide to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a umap 2d embedding projection of the tiles\n",
    "sorted_keys, _ = utils.parse(emb['embedding'])\n",
    "all_X = utils.stack_embedding(emb['embedding'], sorted_keys)\n",
    "reducer = umap.UMAP(n_components=2)\n",
    "embeddings_2d = reducer.fit_transform(all_X)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(10, 10))\n",
    "ax.scatter(embeddings_2d[:,0], embeddings_2d[:,1], s=0)\n",
    "\n",
    "for (x0, y0), key in zip(embeddings_2d, sorted_keys):\n",
    "    tile = OffsetImage(emb['tiles'][key], zoom=0.3)\n",
    "    ab = AnnotationBbox(tile, (x0, y0), frameon=False)\n",
    "    ax.add_artist(ab)\n",
    "\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embedded Space\n",
    "\n",
    "Great! Now let's pull out a sample over the entire dataset and cluster the embeddings to explore the approximate structure of the feature space represented by the embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [e for e in embeddings['images']]\n",
    "all_X = []\n",
    "res_df = []\n",
    "for image in images:\n",
    "    skeys, locdict = utils.parse(image['embedding'])\n",
    "    tiles = utils.tile_thumbnail(image)\n",
    "    image['tiles'] = tiles\n",
    "    sample_keys = np.random.choice(skeys, size=min(1000, len(skeys)), replace=False)\n",
    "    \n",
    "    image['sample_keys'] = sample_keys\n",
    "    sample_X = utils.stack_embedding(image['embedding'], image['sample_keys'])\n",
    "    df = pd.DataFrame({\"sample_key\": image['sample_keys']})\n",
    "    df[\"image_id\"] = image[\"image_id\"]\n",
    "    all_X.append(sample_X)\n",
    "    res_df.append(df)\n",
    "    \n",
    "sample_df = pd.concat(res_df)\n",
    "sample_df['index'] = np.arange(len(sample_df))\n",
    "all_X = np.concatenate(all_X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can zoom in and visualize sample tiles from some of the clusters to confirm that the model is distinguishing relevant tissue types and structures. We’ll again use the low resolution thumbnails returned by the API to generate downsampled versions of the tiles. While these images are lower resolution (32x32 at 7 mpp) than the patches seen by the foundation model (224x224 at the requested 1 mpp), it is easy to see that the model separates the dataset into coherent tissue & stain combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 20\n",
    "cluster_membership = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit_transform(all_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 10\n",
    "for cluster in [1,5,7,9,11,15,17,19]:\n",
    "    top_n = cluster_membership[:,cluster].argsort()[:n_rows*n_rows]\n",
    "    cluster_df = sample_df.iloc[top_n]\n",
    "    print(cluster)\n",
    "    fig, axx = plt.subplots(n_rows,n_rows,figsize=(10, 10))\n",
    "    for i in range(n_rows*n_rows):\n",
    "        ax = axx[i//n_rows, i%n_rows]\n",
    "        ax.axis('off')\n",
    "        # get the thumbnail for the key\n",
    "        key = cluster_df['sample_key'].values[i]\n",
    "        image_id = cluster_df['image_id'].values[i]\n",
    "        emb = [e for e in embeddings['images'] if e['image_id'] == image_id][0]\n",
    "        tile = emb['tiles'][key]\n",
    "        ax.imshow(tile)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s all there is to it! That’s how easy it is to cluster and explore your data with foundation models using Concentriq® Embeddings. Hello computational pathology world.\n",
    "\n",
    "## Foundation models could be at your fingertips\n",
    "\n",
    "Concentriq® Embeddings removes the barriers between researchers and valuable insights. Concentriq® Embeddings makes grabbing features with a foundation model as easy as grabbing a cup of coffee. Researchers can now bypass the cumbersome steps of downloading massive WSI files, wrestling with poorly maintained software, and keeping track of thousands if not millions of cropped image files that might be outdated as soon as they hit the hard drive. No heavy compute, no GPUs, no piles of harddrives. You get straight to building your models. With Concentriq® Embeddings, transition seamlessly from slides to extracted features, ready for downstream task development. The barrier between your data and your models is no more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
