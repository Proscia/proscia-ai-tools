{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Revolutionizing Model Development in Digital Pathology: From Weeks to Minutes\n",
                "\n",
                "The journey of developing AI systems within the realm of digital pathology presents extreme challenges, primarily due to the complexity of managing and processing vast datasets. Traditionally, transforming this data into a usable format for model development was a cumbersome and time-intensive task. With the rise of foundation embedders (models trained on huge corpora of images), the initial steps of model development have been dramatically streamlined.\n",
                "\n",
                "In this post, we explore how Proscia's Concentriq® Embeddings platform transforms the way data scientists and researchers approach AI-driven projects. In this post, we demonstrate the power and simplicity of model development. In this case, we are building a tumor segmentation model. Remarkably, this process requires nothing more than a standard laptop, eliminating the need for costly GPU infrastructure and the logistical complexities associated with handling terabytes of training data. Join us as we showcase how Concentriq® Embeddings accelerates the path from concept to execution, making sophisticated model development accessible and expedient.\n",
                "\n",
                "\n",
                "# CAMELYON17 tumor segmentation using Concentriq® Embeddings\n",
                "\n",
                "The [Camelyon17 dataset](https://camelyon17.grand-challenge.org/) consists of whole slide images (WSIs) in TIFF format collected from 5 medical centers in the Netherlands. Lesion-level annotations are provided for 100 slides, creating an ideal example to demonstrate how to use Concentriq® Embeddings to quickly build a model for tumor segmentation. We’ll accomplish this by performing classification on high-resolution tile embeddings. \n",
                "\n",
                "After loading the dataset into Concentriq® for Research, embedding is initiated via a simple API job request. When the job is done, we download the embeddings, which are stored as safetensors, streamlining operations with a standard format. The embeddings are much smaller than the original WSIs at approximately a factor of 256 compression, enabling straightforward manipulation and analysis even on standard laptops.\n",
                "\n",
                "In this notebook, we will illustrate how to:\n",
                "- Generate embeddings at 1 micron per pixel (mpp, approx. 10X) using the [DINOv2 model](https://huggingface.co/docs/transformers/main/en/model_doc/dinov2)\n",
                "- Load embeddings and labels\n",
                "- Define and train a simple multi-layer perceptron (MLP) pytorch model\n",
                "- Evaluate patch-level performance\n",
                "- Visualize predictions with heatmaps\n",
                "\n",
                "We take this simple approach (no deep learning involved) to illustrate of the power of features derived from even a foundation model trained only on natural images and to show that Concentriq® Embeddings can be used by anyone with a little programming know-how. (Note, that Concentriq® Embeddings does support several pathology-specific models)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import cv2\n",
                "import imageio\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import os\n",
                "import pandas as pd\n",
                "from PIL import Image\n",
                "\n",
                "from utils.client import ClientWrapper as Client\n",
                "from utils import utils\n",
                "\n",
                "Image.MAX_IMAGE_PIXELS = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "email = os.getenv(\"CONCENTRIQ_EMAIL\")\n",
                "pwd = os.getenv(\"CONCENTRIQ_PASSWORD\")\n",
                "endpoint = os.getenv(\"CONCENTRIQ_ENDPOINT_URL\")\n",
                "\n",
                "# To use CPU instead of GPU, set `device` parameter to `\"cpu\"`\n",
                "ce_api_client = Client(url=endpoint, email=email, password=pwd, device=0)\n",
                "ce_api_client"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Let's get some embeddings\n",
                "\n",
                "Now let's embed the Camelyon17 repo (stored on Concentriq® for Research with repo ID 2784) at 1 mpp resolution using the default (DINOv2) model, and print out the ticket ID. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "repository_ids = [2784]\n",
                "ticket_id = ce_api_client.embed_repos(ids=repository_ids, model=\"facebook/dinov2-base\", mpp=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optionally load embeddings from a previously created ticket\n",
                "# ticket_id = 'a0660805-26dc-4092-a389-b09113e7e64c' # repo 2784 at 1mpp\n",
                "embeddings = ce_api_client.get_embeddings(ticket_id)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "len(embeddings['images'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Congratulations, you're now a foundation model wizard. You have your embeddings in just a few lines of code!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Let's match embeddings to metadata\n",
                "We're going to want to associate embeddings with the corresponding image names. Concentriq® Embeddings already links images to an ID, so we just want to match those Concentriq® `image_id`s with `image_names`. Other metadata can be linked this way too. Here we're just pulling this data using the `ConcentriqLSClient` using the `get_images_in_a_repo` method. The `ConcentriqLSClient` uses the same authentication credentials as the embeddings API."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from utils.client import ConcentriqLSClient\n",
                "\n",
                "cls = ConcentriqLSClient(url=endpoint, email=email, password=pwd)\n",
                "concentriq_metadata = cls.get_images_in_a_repo(repository_ids[0])\n",
                "\n",
                "columns = [\"id\", \"mppx\", \"mppy\", \"imgWidth\", \"imgHeight\", \"objectivePower\", \"slideName\"]\n",
                "concentriq_metadata = pd.DataFrame(concentriq_metadata[\"images\"])[columns]\n",
                "concentriq_metadata.rename(columns={\"id\": \"image_id\"}, inplace=True)\n",
                "\n",
                "concentriq_metadata[\"image_base_name\"] = concentriq_metadata[\"slideName\"].apply(lambda x: x.split(\".\")[0])\n",
                "print(concentriq_metadata.shape)\n",
                "concentriq_metadata.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The retrieved embedding file includes metadata about the embeddings job we submitted and each WSI processed. The embeddings themselves are loaded as a dictionary of tensors with keys in \"Y_X\" format indicating the grid index of the embedded patch.\n",
                "\n",
                "Here we print the information corresponding to each WSI. This includes the parameters supplied along with implicit attributes of the data, like the foundation model’s native patch size, as well as all of the other spatial information associated with each WSI’s tile embeddings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for key, value in embeddings['images'][0].items():\n",
                "    if key not in ['embedding', 'thumbnail']:\n",
                "        print(f\"{key}: {value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ... And add tile-level labels\n",
                "Now we want to match each tile embedding with its corresponding label. To do this, we can create a dataframe with one row per embedded tile along with the grid location and the mask value for that tile.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "datapath = \"data/camelyon17/masks/\"\n",
                "reslist = []\n",
                "for i, row in concentriq_metadata.iterrows():\n",
                "    image_id = row[\"image_id\"]\n",
                "    image_base_name = row[\"image_base_name\"]\n",
                "    emb = [e for e in embeddings['images'] if e[\"image_id\"] == image_id][0]\n",
                "    tile_res_mask_path = os.path.join(datapath, f\"{image_base_name}_mask.png\")\n",
                "    tile_res_mask = imageio.v2.imread(tile_res_mask_path)    \n",
                "    # For each image, create one row per tile in the mask\n",
                "    for i in range(emb[\"grid_rows\"]):\n",
                "        for j in range(emb[\"grid_cols\"]):\n",
                "            mask_value = tile_res_mask[i, j]\n",
                "            res = {\"image_id\": image_id,\n",
                "                   \"image_base_name\": image_base_name,\n",
                "                   \"label\": mask_value,\n",
                "                   \"row\": i,\n",
                "                   \"col\": j,\n",
                "                   \"embedding\": emb[\"embedding\"][f\"{i}_{j}\"]}\n",
                "            reslist.append(res)\n",
                "dataset_df = pd.DataFrame(reslist)\n",
                "dataset_df.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_df['label'].value_counts()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Restrict the dataset to tiles containing tissue."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_df = dataset_df[dataset_df['label'] > 0]\n",
                "dataset_df.index = range(len(dataset_df))\n",
                "dataset_df.shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Create training and test splits\n",
                "Split the dataset into 80% train and 20% test while stratifying over images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.random.seed(12356)\n",
                "test_ids = np.random.choice(dataset_df['image_id'].unique(), 20, replace=False)\n",
                "train = dataset_df[~dataset_df['image_id'].isin(test_ids)].copy()\n",
                "test = dataset_df[dataset_df['image_id'].isin(test_ids)].copy()\n",
                "del dataset_df\n",
                "train['image_id'].nunique(), test['image_id'].nunique()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Train a model\n",
                "\n",
                "Next, we’ll build a simple classifier over tile embeddings to produce a low-resolution segmentation map distinguishing tumor from normal tissue. This extremely simplified feedforward, multilayer perceptron model and training procedure is adapted from this [pytorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import DataLoader, Dataset\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "import torch\n",
                "\n",
                "class CamelyonDataset(Dataset):\n",
                "    def __init__(self, df: pd.DataFrame):\n",
                "        self.df = df\n",
                "        self.df.index = range(self.df.shape[0])\n",
                "\n",
                "    def __len__(self):\n",
                "        return self.df.shape[0]\n",
                "\n",
                "    def __getitem__(self, idx: int):\n",
                "        row = self.df.iloc[idx]\n",
                "        embedding = row['embedding']\n",
                "        label = row['label']\n",
                "        return embedding, torch.Tensor([label-1]).long().to('cuda')\n",
                "    \n",
                "train_dataset = CamelyonDataset(train)\n",
                "test_dataset = CamelyonDataset(test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
                "testloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
                "\n",
                "class Net(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.fc1 = nn.Linear(768, 120)\n",
                "        self.fc2 = nn.Linear(120, 84)\n",
                "        self.fc3 = nn.Linear(84, 2)\n",
                "        \n",
                "        self.dropout1 = nn.Dropout(0.2)\n",
                "        self.dropout2 = nn.Dropout(0.2)\n",
                "        self.dropout3 = nn.Dropout(0.2)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
                "        x = self.dropout1(x)\n",
                "        x = F.relu(self.fc1(x))\n",
                "        x = self.dropout2(x)\n",
                "        x = F.relu(self.fc2(x))\n",
                "        x = self.dropout3(x)\n",
                "        x = self.fc3(x)\n",
                "        return x\n",
                "\n",
                "\n",
                "net = Net().to('cuda')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for epoch in range(5):  # loop over the dataset multiple times\n",
                "    running_loss = 0.0\n",
                "    for i, data in enumerate(trainloader, 0):\n",
                "        # get the inputs; data is a list of [inputs, labels]\n",
                "        inputs, labels = data\n",
                "        # zero the parameter gradients\n",
                "        optimizer.zero_grad()\n",
                "\n",
                "        # forward + backward + optimize\n",
                "        outputs = net(inputs)\n",
                "        loss = criterion(outputs, labels.squeeze())\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        # print statistics\n",
                "        running_loss += loss.item()\n",
                "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
                "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
                "            running_loss = 0.0\n",
                "\n",
                "print('Finished Training')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluate\n",
                "Great, now we’ve built a Camelyon17 segmentation model in minutes. Let’s assess the performance of the model on the test set. First, we’ll just look at tilewise accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "correct = 0\n",
                "total = 0\n",
                "preds = []\n",
                "net.eval()\n",
                "with torch.no_grad():\n",
                "    for data in testloader:\n",
                "        images, labels = data\n",
                "        # calculate outputs by running images through the network\n",
                "        outputs = net(images)\n",
                "        preds.append(outputs.data)\n",
                "        # the class with the highest energy is what we choose as prediction\n",
                "        _, predicted = torch.max(outputs.data, 1)\n",
                "        total += labels.squeeze().size(0)\n",
                "        correct += (predicted == labels.squeeze()).sum().item()\n",
                "\n",
                "print(f'Accuracy of the network on test images: {100 * correct // total} %')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Okay, that looks good, but let’s see what other metrics look like."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pred = torch.cat(preds)\n",
                "tumor_pred = torch.softmax(pred, dim=1)[:, 1]\n",
                "test['pred'] = tumor_pred.cpu().numpy()\n",
                "test['pred_label'] = torch.argmax(pred, dim=1).cpu().numpy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "utils.calculate_boolean_metrics(gt = test['label']-1, pred = test['pred_label'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "iou = utils.calculate_iou(gt = test['label']-1, pred = test['pred_label'])\n",
                "dice = utils.calculate_dice(gt = test['label']-1, pred = test['pred_label'])\n",
                "iou, dice"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "So overall the model has very high specificity and sensitivity is lower, but this seems very respectable for a tiny model that took us under an hour to put together on a laptop, including time spent fetching the data from Concentriq®."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Visualize\n",
                "Let’s see what our segmentations look like. \n",
                "\n",
                "For this we'll grab thumbnail images at 7 microns per pixel from the thumbnails endpoint.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "thumbnail_ticket_id = ce_api_client.thumbnail_repos(ids=repository_ids)\n",
                "print(thumbnail_ticket_id)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "thumbnails = ce_api_client.get_thumbnails(thumbnail_ticket_id, load_thumbnails=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# link the thumbnails to the embeddings\n",
                "for emb in embeddings['images']:\n",
                "    thumbnail_dict = [thumb for thumb in thumbnails['thumbnails'] if thumb['image_id'] == emb['image_id']][0]\n",
                "    emb.update(thumbnail_dict)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can plot the ground truth masks next to model predictions and visualize tumor region predictions as heat maps overlaid on image thumbnails."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ious = []\n",
                "dices = []\n",
                "\n",
                "gb = test.groupby('image_id')\n",
                "for image_id, image_df in gb:\n",
                "    if (image_df['label'] == 2).sum() < 10:\n",
                "        # skip images with no or very little tumor\n",
                "        continue\n",
                "    \n",
                "    emb = [e for e in embeddings['images'] if e[\"image_id\"] == image_id][0]\n",
                "    mat = np.zeros((emb['grid_rows'], emb['grid_cols']))\n",
                "    for i, row in image_df.iterrows():\n",
                "        mat[row['row'], row['col']] = row['pred']\n",
                "    image_base_name = image_df['image_base_name'].values[0]\n",
                "    fig, axx = plt.subplots(1,3)\n",
                "    axx[0].imshow(mat)\n",
                "    axx[0].set_title(\"Predicted\")\n",
                "    axx[0].axis('off')\n",
                "    \n",
                "    gt_mask = imageio.v2.imread(os.path.join(datapath, f\"{image_base_name}_mask.png\"))==2\n",
                "    axx[1].imshow(gt_mask)\n",
                "    axx[1].set_title(\"Ground Truth\")\n",
                "    axx[1].axis('off')\n",
                "    axx[2].imshow(emb['thumbnail'])\n",
                "    axx[2].set_title(\"Thumbnail\")\n",
                "    axx[2].axis('off')\n",
                "    plt.show()\n",
                "    pred_thumb = cv2.resize(mat, (emb['thumbnail'].shape[1], emb['thumbnail'].shape[0]), 0, 0, interpolation=cv2.INTER_NEAREST)\n",
                "\n",
                "    fig, ax = plt.subplots(1,1, figsize=(15,15))\n",
                "    ## create contours from the mask\n",
                "    gt_mask_thumb_res = cv2.resize(gt_mask.astype('uint8'), (emb['thumbnail'].shape[1], emb['thumbnail'].shape[0]), 0, 0, interpolation=cv2.INTER_NEAREST)\n",
                "    contours, _ = cv2.findContours(gt_mask_thumb_res.astype('uint8'), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
                "    thumbnail = emb['thumbnail'].copy()\n",
                "    thumbnail = cv2.drawContours(thumbnail, contours, -1, (1, 1, 0), 6)\n",
                "    \n",
                "    ax.imshow(thumbnail)\n",
                "    ax.imshow(-pred_thumb, alpha=pred_thumb/pred_thumb.max(), cmap='RdYlBu', vmax=0, vmin=-1)\n",
                "    ax.set_title(\"Prediction Heatmap\")\n",
                "    ax.axis('off')\n",
                "    \n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "These visualizations seem to tell the same story as our metrics - imperfect, but fairly respectable performance for a small model and low effort. It’s that simple to make use of foundation models for segmentation with Concentriq® Embeddings.\n",
                "\n",
                "# Conclusion\n",
                "Concentriq® Embeddings revolutionizes the development of histopathology algorithms, streamlining the journey from prototyping to production. By harnessing the power of foundation model embeddings, the platform significantly speeds up every aspect of model development. Whether it's clustering, segmentation, or classification, Concentriq® Embeddings provides a robust foundation for AI-driven innovations in histopathology, turning processes that once took weeks into tasks that can be accomplished in less than an hour."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
